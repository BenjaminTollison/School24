{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\benja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Num GPUs Available:  0\n",
      "[]\n",
      "WARNING:tensorflow:From c:\\Users\\benja\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from tqdm.notebook import tqdm\n",
    "from keras.layers import Dense\n",
    "from numpy import pi\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "def createTraj():\n",
    "    fhat = Sequential()\n",
    "    fhat.add(Dense(50, activation=\"sigmoid\", input_dim=1))\n",
    "    fhat.add(Dense(1))\n",
    "    return fhat\n",
    "\n",
    "x = createTraj()\n",
    "y = createTraj()\n",
    "velocity = createTraj()\n",
    "theta = createTraj()\n",
    "thrust = createTraj()\n",
    "moment = createTraj()\n",
    "\n",
    "initial_time = 0\n",
    "final_time = 1\n",
    "number_of_points = 20\n",
    "normalize_time = tf.cast(tf.linspace(initial_time,final_time,number_of_points),dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errors(t):\n",
    "  with tf.GradientTape(persistent=True) as tape1:\n",
    "    tape1.watch(t)\n",
    "    with tf.GradientTape(persistent=True) as tape2:\n",
    "        tape2.watch(t)\n",
    "        X = x(t)\n",
    "        Y = y(t)\n",
    "        TH = theta(t)\n",
    "        V = velocity(t)\n",
    "    \n",
    "    Xd = tape2.gradient(X, t)\n",
    "    Yd = tape2.gradient(Y, t)\n",
    "\n",
    "  Xdd = tape1.gradient(Xd, t)\n",
    "  Ydd = tape1.gradient(Yd, t)\n",
    "\n",
    "  # Compute the derivatives\n",
    "  THd = tape1.gradient(TH, t)\n",
    "  Vd = tape1.gradient(V, t)\n",
    "\n",
    "  # Error in dynamics\n",
    "  e_xdot = tf.reduce_sum((tf.reshape(Xdd, shape=(number_of_points, 1)) - Vd * tf.cos(TH)) ** 2)\n",
    "  e_ydot = tf.reduce_sum((tf.reshape(Ydd, shape=(number_of_points, 1)) - Vd * tf.sin(TH)) ** 2)\n",
    "\n",
    "  # Error in initial condition\n",
    "  x0 = 0\n",
    "  y0 = 0\n",
    "  th_0 = 0\n",
    "  x_dot_0 = 0\n",
    "  y_dot_0 = 0\n",
    "  th_dot_0 = 0\n",
    "  eIC = (X[0] - x0) ** 2 + (Y[0] - y0) ** 2 + (TH[0]) ** 2 + V[0] ** 2  # start from (x0,y0) at rest and pointing east\n",
    "\n",
    "  # Error in final condition\n",
    "  xf = 1\n",
    "  yf = 1\n",
    "  th_f = 0\n",
    "  x_dot_f = 0\n",
    "  y_dot_f = 0\n",
    "  th_dot_0 = 0\n",
    "  eFC = (X[-1] - xf) ** 2 + (Y[-1] - yf) ** 2 + (TH[-1] - np.pi) ** 2 + V[-1] ** 2  # end at (xf,yf) at rest and pointing west\n",
    "\n",
    "  return e_xdot + e_ydot + 10 * eIC + 10 * eFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_counter = 0\n",
    "while errors(T) >= 0.001:\n",
    "    nIter = 400\n",
    "    run_counter += 1\n",
    "    if run_counter == 21:\n",
    "        print('Failed to converge')\n",
    "        break\n",
    "    if errors(T) <= 1:\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.01)\n",
    "    elif errors(T) <= 0.5:\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "    elif errors(T) <= 0.1:\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0001)\n",
    "    elif errors(T) <= 0.01:\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.00001)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.1)\n",
    "    for i in tqdm(range(nIter),desc=f\"Training run {run_counter}\"):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            e = errors(T)\n",
    "        if e <= 0.001:\n",
    "            print('Finish Training')\n",
    "            break\n",
    "        # if i%50 == 0:    \n",
    "            # print(f'iter: {i}, error: {e[0]}')\n",
    "    \n",
    "        # Update parameters in x\n",
    "        grads = tape.gradient(e, x.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, x.trainable_variables))\n",
    "\n",
    "        # Update parameters in y\n",
    "        grads = tape.gradient(e, y.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, y.trainable_variables))\n",
    "\n",
    "        # Update parameters in v\n",
    "        grads = tape.gradient(e, v.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, v.trainable_variables))\n",
    "\n",
    "        # Update parameters in th\n",
    "        grads = tape.gradient(e, th.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, th.trainable_variables))\n",
    "    print(f\"Last error: {round(float(errors(T)[0]),4)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
