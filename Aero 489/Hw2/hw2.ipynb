{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 25 is different from 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 95\u001b[0m\n\u001b[1;32m     91\u001b[0m alpha_guess \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mhstack([alpha_x,alpha_y,alpha_theta,alpha_T,alpha_M])\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Minimize the error function\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mError\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_guess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstraints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# Extract the optimized coefficients\u001b[39;00m\n\u001b[1;32m     98\u001b[0m optimized_alpha \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mx\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/scipy/optimize/_minimize.py:722\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    719\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_cobyla(fun, x0, args, constraints, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    720\u001b[0m                            bounds\u001b[38;5;241m=\u001b[39mbounds, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mslsqp\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 722\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_slsqp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mconstraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrust-constr\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    725\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_trustregion_constr(fun, x0, args, jac, hess, hessp,\n\u001b[1;32m    726\u001b[0m                                        bounds, constraints,\n\u001b[1;32m    727\u001b[0m                                        callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/scipy/optimize/_slsqp_py.py:383\u001b[0m, in \u001b[0;36m_minimize_slsqp\u001b[0;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    380\u001b[0m     xu[infbnd[:, \u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# ScalarFunction provides function and gradient evaluation\u001b[39;00m\n\u001b[0;32m--> 383\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_bounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# gh11403 SLSQP sometimes exceeds bounds by 1 or 2 ULP, make sure this\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;66;03m# doesn't get sent to the func/grad evaluator.\u001b[39;00m\n\u001b[1;32m    388\u001b[0m wrapped_fun \u001b[38;5;241m=\u001b[39m _clip_x_for_func(sf\u001b[38;5;241m.\u001b[39mfun, new_bounds)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/scipy/optimize/_optimize.py:402\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    398\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:166\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:262\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:163\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/scipy/optimize/_differentiable_functions.py:145\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[0;32mIn[1], line 70\u001b[0m, in \u001b[0;36mError\u001b[0;34m(alpha)\u001b[0m\n\u001b[1;32m     67\u001b[0m alpha_T \u001b[38;5;241m=\u001b[39m alpha[\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(t):\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(t)]\n\u001b[1;32m     68\u001b[0m alpha_M \u001b[38;5;241m=\u001b[39m alpha[\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(t):]\n\u001b[0;32m---> 70\u001b[0m error_x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum((\u001b[43mmass\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msecond_derivative_basis_functions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43malpha_x_slice\u001b[49m \u001b[38;5;241m-\u001b[39m \\\n\u001b[1;32m     71\u001b[0m                   basis_functions \u001b[38;5;241m@\u001b[39m alpha_T \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mcos(basis_functions \u001b[38;5;241m@\u001b[39m alpha_theta_slice))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     72\u001b[0m error_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum((mass \u001b[38;5;241m*\u001b[39m second_derivative_basis_functions \u001b[38;5;241m@\u001b[39m alpha_y_slice \u001b[38;5;241m-\u001b[39m \\\n\u001b[1;32m     73\u001b[0m                   basis_functions \u001b[38;5;241m@\u001b[39m alpha_T \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msin(basis_functions \u001b[38;5;241m@\u001b[39m alpha_theta_slice))\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     74\u001b[0m error_M \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum((inertia_z \u001b[38;5;241m*\u001b[39m second_derivative_basis_functions \u001b[38;5;241m@\u001b[39m alpha_theta_slice \u001b[38;5;241m-\u001b[39m basis_functions \u001b[38;5;241m@\u001b[39m alpha_M)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 25 is different from 5)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def generate_polynomial_basis(t, degree):\n",
    "    basis_functions = []\n",
    "    for d in range(degree + 1):\n",
    "        basis_functions.append(t**d)\n",
    "    basis_functions = np.array(basis_functions).T\n",
    "    derivative_basis_functions = np.zeros_like(basis_functions)\n",
    "    for d in range(degree):\n",
    "        derivative_basis_functions[:, d] = (d + 1) * basis_functions[:, d + 1]\n",
    "    second_derivative_basis_functions = np.zeros_like(basis_functions)\n",
    "    for d in range(degree - 1):\n",
    "        second_derivative_basis_functions[:, d] = (d + 1) * derivative_basis_functions[:, d + 1]\n",
    "    return basis_functions, derivative_basis_functions, second_derivative_basis_functions\n",
    "t = np.linspace(0, 1, 100)  # Time vector\n",
    "def circle_constraint_1(variables):\n",
    "    x, y = variables[0],variables[2]  # Get final x, y from final conditions\n",
    "    x1, y1 = 2.0, 5.0  # Center of the circle\n",
    "    r = 2.0  # Radius of the circle\n",
    "    return (x - x1)**2 + (y - y1)**2 - r**2  # Ensure the point is outside the circle\n",
    "def circle_constraint_2(variables):\n",
    "    x, y = variables[0],variables[2]  # Get final x, y from final conditions\n",
    "    x1, y1 = 3.0, 8.0  # Center of the circle\n",
    "    r = 2.0  # Radius of the circle\n",
    "    return (x - x1)**2 + (y - y1)**2 - r**2  # Ensure the point is outside the circle\n",
    "t = np.linspace(0, 1, 100)  # Time vector\n",
    "degree = 4  # Degree of the polynomial\n",
    "basis_functions, derivative_basis_functions, second_derivative_basis_functions = generate_polynomial_basis(t, degree)\n",
    "# print(basis_functions.shape,derivative_basis_functions.shape,second_derivative_basis_functions.shape)\n",
    "alpha_x,alpha_y,alpha_theta,alpha_T,alpha_M = np.zeros(5),np.zeros(5),np.zeros(5),np.zeros(5),np.zeros(5)\n",
    "constraints = ({'type': 'ineq', 'fun': lambda variables: basis_functions@alpha_T},  # T >= 0\n",
    "               {'type': 'ineq', 'fun': lambda variables: 1 - basis_functions@alpha_T},  # T <= 1\n",
    "               {'type': 'ineq', 'fun': lambda variables: -1 - basis_functions@alpha_M},  # M >= -1\n",
    "               {'type': 'ineq', 'fun': lambda variables: basis_functions@alpha_M - 1}, # M <= 1\n",
    "               {'type': 'ineq', 'fun': circle_constraint_1},  # Constraint to keep point outside the circle 1\n",
    "               {'type': 'ineq', 'fun': circle_constraint_2})  # Constraint to keep point outside the circle 2\n",
    "initial_conditions = np.array([0.0,0.0,0.0,0.0,0.0]) # x,y,theta,T,M\n",
    "final_conditions = np.array([0.0,0.0,0.0,0.0,0.0]) # x,y,theta,T,M\n",
    "# def Error(time_index):\n",
    "  # mass = 1\n",
    "  # inertia_z = 1\n",
    "  # error_x = (mass*second_derivative_basis_functions@alpha_x[time_index] - \\\n",
    "    # basis_functions@alpha_T[time_index]*np.cos(basis_functions@alpha_theta[time_index]))**2\n",
    "  # error_y = (mass*second_derivative_basis_functions@alpha_y[time_index] - \\\n",
    "    # basis_functions@alpha_T[time_index]*np.sin(basis_functions@alpha_theta[time_index]))**2\n",
    "  # error_M = (inertia_z*second_derivative_basis_functions@alpha_theta - basis_functions@alpha_M)**2\n",
    "  # error_IC = ([basis_functions@alpha_x][0] - initial_conditions[0])**2 + \\\n",
    "    # ([basis_functions@alpha_y][0] - initial_conditions[1])**2 + \\\n",
    "    # ([basis_functions@alpha_theta][0] - initial_conditions[2])**2 + \\\n",
    "    # ([basis_functions@alpha_T][0] - initial_conditions[3])**2 + \\\n",
    "    # ([basis_functions@alpha_M][0] - initial_conditions[4])**2\n",
    "  # error_FC = ([basis_functions@alpha_x][-1] - final_conditions[0])**2 + \\\n",
    "    # ([basis_functions@alpha_y][-1] - final_conditions[1])**2 + \\\n",
    "    # ([basis_functions@alpha_theta][-1] - final_conditions[2])**2 + \\\n",
    "    # ([basis_functions@alpha_T][-1] - final_conditions[3])**2 + \\\n",
    "    # ([basis_functions@alpha_M][-1] - final_conditions[4])**2\n",
    "  # J = error_x + error_y + error_M + error_IC + error_FC\n",
    "  # return J\n",
    "def Error(alpha):\n",
    "  mass = 1\n",
    "  inertia_z = 1\n",
    "  alpha_x_slice = alpha[:len(t)]\n",
    "  alpha_y_slice = alpha[len(t):2*len(t)]\n",
    "  alpha_theta_slice = alpha[2*len(t):3*len(t)]\n",
    "  alpha_T = alpha[3*len(t):4*len(t)]\n",
    "  alpha_M = alpha[4*len(t):]\n",
    "\n",
    "  error_x = np.sum((mass * second_derivative_basis_functions @ alpha_x_slice - \\\n",
    "                    basis_functions @ alpha_T * np.cos(basis_functions @ alpha_theta_slice))**2)\n",
    "  error_y = np.sum((mass * second_derivative_basis_functions @ alpha_y_slice - \\\n",
    "                    basis_functions @ alpha_T * np.sin(basis_functions @ alpha_theta_slice))**2)\n",
    "  error_M = np.sum((inertia_z * second_derivative_basis_functions @ alpha_theta_slice - basis_functions @ alpha_M)**2)\n",
    "\n",
    "  error_IC = np.sum((basis_functions @ alpha_x_slice - initial_conditions[0])**2 + \\\n",
    "                    (basis_functions @ alpha_y_slice - initial_conditions[1])**2 + \\\n",
    "                    (basis_functions @ alpha_theta_slice - initial_conditions[2])**2 + \\\n",
    "                    (basis_functions @ alpha_T - initial_conditions[3])**2 + \\\n",
    "                    (basis_functions @ alpha_M - initial_conditions[4])**2)\n",
    "  error_FC = np.sum((basis_functions @ alpha_x_slice - final_conditions[0])**2 + \\\n",
    "                    (basis_functions @ alpha_y_slice - final_conditions[1])**2 + \\\n",
    "                    (basis_functions @ alpha_theta_slice - final_conditions[2])**2 + \\\n",
    "                    (basis_functions @ alpha_T - final_conditions[3])**2 + \\\n",
    "                    (basis_functions @ alpha_M - final_conditions[4])**2)\n",
    "  J = error_x + error_y + error_M + error_IC + error_FC\n",
    "  return J\n",
    "\n",
    "\n",
    "# Initial guess for alpha\n",
    "alpha_guess = np.hstack([alpha_x,alpha_y,alpha_theta,alpha_T,alpha_M])\n",
    "\n",
    "\n",
    "# Minimize the error function\n",
    "result = minimize(Error, alpha_guess, constraints=constraints)\n",
    "\n",
    "# Extract the optimized coefficients\n",
    "optimized_alpha = result.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 12:44:21.178850: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-04 12:44:21.346571: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-04 12:44:21.346687: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-04 12:44:21.368852: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-04 12:44:21.431843: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-04 12:44:22.297907: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 12:44:23.163494: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:f3:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-04 12:44:23.357006: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:f3:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-04 12:44:23.357041: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:f3:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from tqdm.notebook import tqdm\n",
    "from keras.layers import Dense\n",
    "from numpy import pi\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 12:44:29.452438: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:f3:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-04 12:44:29.452536: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:f3:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-04 12:44:29.452553: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:f3:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-04 12:44:29.682774: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:f3:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-04 12:44:29.682905: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:f3:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-04 12:44:29.682915: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-03-04 12:44:29.682960: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:f3:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-04 12:44:29.683050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Ti Laptop GPU, pci bus id: 0000:f3:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Design the neural network\n",
    "def createTraj():\n",
    "    fhat = Sequential()\n",
    "    fhat.add(Dense(50, activation=\"sigmoid\", input_dim=1))\n",
    "    fhat.add(Dense(1))\n",
    "    return fhat\n",
    "\n",
    "x = createTraj()\n",
    "y = createTraj()\n",
    "T = createTraj()\n",
    "M = createTraj()\n",
    "theta = createTraj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "T0 = 0\n",
    "Tf = 1\n",
    "nPts = 20\n",
    "T = tf.cast(tf.linspace(0,Tf,nPts),dtype=tf.float32)\n",
    "\n",
    "def errors(t):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(t)\n",
    "        X = x(t)\n",
    "        Y = y(t)\n",
    "    mass = 1\n",
    "    inertia_z = 1\n",
    "    Xd = tape.gradient(X,t)\n",
    "    Xdd = tape.gradient(Xd,t)\n",
    "    Yd = tape.gradient(Y,t)\n",
    "    Ydd = tape.gradient(Yd,t)\n",
    "    TH = theta(t)\n",
    "    TH_d = tape.gradient(TH,t)\n",
    "    TH_dd = tape.gradient(TH_d,t)\n",
    "    Thrust  = T(t)\n",
    "    Moment  = M(t)\n",
    "    \n",
    "    # Error in dynamics\n",
    "    e_xddot = tf.reduce_sum((tf.reshape(Xdd,shape=(nPts,1)) - Thrust*tf.cos(TH)/mass)**2)\n",
    "    e_yddot = tf.reduce_sum((tf.reshape(Ydd,shape=(nPts,1)) - T*tf.sin(TH)/mass)**2)\n",
    "    e_thetaddot = tf.reduce_sum((tf.reshape(TH_dd,shape=(nPts,1)) - Moment/inertia_z)**2)\n",
    "\n",
    "    # Error in initial condition\n",
    "    x0 = 0; y0 = 0; th_0 = 0; x_dot_0 = 0; y_dot_0 = 0; th_dot_0 = 0;\n",
    "    eIC = (X[0] - x0)**2  + (Y[0] - y0)**2  + (TH[0])**2 + Thrust[0]**2 + Moment[0]**2# start from (x0,y0) at rest and pointing east\n",
    "\n",
    "    # Error in final condition\n",
    "    xf = 1; yf = 1; th_f = 0; x_dot_f = 0; y_dot_f = 0; th_dot_0 = 0;\n",
    "    eFC = (X[-1] - xf)**2 + (Y[-1] - yf)**2 + (TH[-1]-pi)**2 + Thrust[-1]**2 + Moment[-1]**2 # end at (xf,yf) at rest and pointing west \n",
    "    \n",
    "    return e_xddot + e_yddot + 10*eIC + 10*eFC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument `target` should be a list or nested structure of Tensors, Variables or CompositeTensors to be differentiated, but received None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m run_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43merrors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m:\n\u001b[1;32m      3\u001b[0m     nIter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m\n\u001b[1;32m      4\u001b[0m     run_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[8], line 19\u001b[0m, in \u001b[0;36merrors\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m     17\u001b[0m TH \u001b[38;5;241m=\u001b[39m theta(t)\n\u001b[1;32m     18\u001b[0m TH_d \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(TH,t)\n\u001b[0;32m---> 19\u001b[0m TH_dd \u001b[38;5;241m=\u001b[39m \u001b[43mtape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTH_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m Thrust  \u001b[38;5;241m=\u001b[39m T(t)\n\u001b[1;32m     21\u001b[0m Moment  \u001b[38;5;241m=\u001b[39m M(t)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py:1023\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     logging\u001b[38;5;241m.\u001b[39mlog_first_n(\n\u001b[1;32m   1012\u001b[0m         logging\u001b[38;5;241m.\u001b[39mWARN, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling GradientTape.gradient on a persistent \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1013\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtape inside its context is significantly less \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient in order to compute higher order \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mderivatives.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1023\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `target` should be a list or nested structure\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m of Tensors, Variables or CompositeTensors to be \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1025\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiated, but received None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1027\u001b[0m flat_targets \u001b[38;5;241m=\u001b[39m composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1028\u001b[0m     nest\u001b[38;5;241m.\u001b[39mflatten(target))\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;66;03m# TODO(b/246997907): Remove this once\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;66;03m# ResourceVariableGradient.get_gradient_components returns the handle.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument `target` should be a list or nested structure of Tensors, Variables or CompositeTensors to be differentiated, but received None."
     ]
    }
   ],
   "source": [
    "run_counter = 0\n",
    "while errors(T) >= 0.001:\n",
    "    nIter = 400\n",
    "    run_counter += 1\n",
    "    if run_counter == 11:\n",
    "        print('Failed to converge')\n",
    "        break\n",
    "    if errors(T) <= 1:\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.01)\n",
    "    elif errors(T) <= 0.5:\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "    elif errors(T) <= 0.1:\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0001)\n",
    "    elif errors(T) <= 0.01:\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.00001)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.1)\n",
    "    for i in tqdm(range(nIter),desc=f\"Training run {run_counter}\"):\n",
    "        with tf.GradientTape(persistent=True) as tape_second_order:\n",
    "          with tf.GradientTape(persistent=True) as tape_first_order:\n",
    "            e = errors(T)\n",
    "          if e <= 0.001:\n",
    "            print('Finish Training')\n",
    "            break\n",
    "        # if i%50 == 0:    \n",
    "            # print(f'iter: {i}, error: {e[0]}')\n",
    "    \n",
    "        # Update parameters in x\n",
    "            grads = tape.gradient(e, x.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, x.trainable_variables))\n",
    "\n",
    "            # Update parameters in y\n",
    "            grads = tape.gradient(e, y.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, y.trainable_variables))\n",
    "\n",
    "            # Update parameters in Thrust\n",
    "            grads = tape.gradient(e, T.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, T.trainable_variables))\n",
    "\n",
    "            # Update parameters in Moment\n",
    "            grads = tape.gradient(e, M.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, M.trainable_variables))\n",
    "\n",
    "            # Update parameters in th\n",
    "            grads = tape.gradient(e, theta.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, theta.trainable_variables))\n",
    "      # Update parameters in x\n",
    "          grads = tape.gradient(e, x.trainable_variables)\n",
    "          optimizer.apply_gradients(zip(grads, x.trainable_variables))\n",
    "\n",
    "          # Update parameters in y\n",
    "          grads = tape.gradient(e, y.trainable_variables)\n",
    "          optimizer.apply_gradients(zip(grads, y.trainable_variables))\n",
    "\n",
    "          # Update parameters in Thrust\n",
    "          grads = tape.gradient(e, T.trainable_variables)\n",
    "          optimizer.apply_gradients(zip(grads, T.trainable_variables))\n",
    "\n",
    "          # Update parameters in Moment\n",
    "          grads = tape.gradient(e, M.trainable_variables)\n",
    "          optimizer.apply_gradients(zip(grads, M.trainable_variables))\n",
    "\n",
    "          # Update parameters in th\n",
    "          grads = tape.gradient(e, theta.trainable_variables)\n",
    "          optimizer.apply_gradients(zip(grads, theta.trainable_variables))\n",
    "    print(f\"Last error: {round(float(errors(T)[0]),4)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
